{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial \n",
    "\n",
    "This tutorial is a demo to show how to use PyTorch to train network for digital recognize with MNIST as datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 check out if your cumputer support GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, you have a GPU!\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if (use_cuda):\n",
    "    print(\"Great, you have a GPU!\")\n",
    "else:\n",
    "    print(\"Life is short -- consider a GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 set a special variable 'device' \n",
    "\n",
    "The 'device' variable denotes what device will the cumpute going on.\n",
    "- If your device has a GPU than, all your compute will be on GPU.\n",
    "- In later code, It will use the function '.to()' to tell PyTorch which device will the compute going on.\n",
    "- It use 'cuda:X' to select the Xth GPU.\n",
    "- And you can ues 'cuda',if you don't care about GPU.\n",
    "- To know the information about your gpu, use 'nvidia-smi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your device will be:  cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if use_cuda else \"cpu\")\n",
    "print('Your device will be: ', device)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perpare Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 get your dataset \n",
    "\n",
    "In this function, you will perpare all your dataset, including data preprocess, data augmentaion and so on.\n",
    "\n",
    "The following code use PyTorch's datasets directly. You will see that the demo use the MNIST provided by PyTorch so that we can push the demo quickly.\n",
    "\n",
    "The function 'transforms' is used for preprocessing and you use some funtion for data augmentaion.\n",
    "\n",
    "In 'transform':\n",
    "    - the 'ToTensor()' will transform the picture to Tensor and scale the value to range 0.0 to 1.0.\n",
    "    - the 'Normalize()' will normalize the picture to range -1.0 to 1.0.\n",
    "\n",
    "See more information in the [offical docs](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "    \n",
    "Than we use 'torchvision.datasets.MNIST' to get the dataset and with the parameter 'transform' we can preprocess dataset. When the parameter 'download' equals to 'True' the PyTorch will download the dataset from internet to 'root'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    './mnist', train = True, \n",
    "    transform = transform,\n",
    "    download = False\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    './mnist', train = False, \n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the size of your dataset using '.size()'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: <class 'torch.Tensor'>\n",
      "train_labels: <class 'torch.Tensor'>\n",
      "test_data: torch.Size([10000, 28, 28])\n",
      "['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_exists', 'download', 'processed_folder', 'raw_folder', 'root', 'target_transform', 'test_file', 'train', 'train_data', 'train_labels', 'training_file', 'transform', 'urls']\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data:\", type(train_data.train_data))\n",
    "print(\"train_labels:\", type(train_data.train_labels))\n",
    "print(\"test_data:\", test_data.test_data.size())\n",
    "print(dir(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 prepare dataloader\n",
    "\n",
    "DataLoader combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset. The iterator contains datas and labels.\n",
    "\n",
    "The 'batch_size' and 'shuffle' is easily understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                          batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 visualize data\n",
    "\n",
    "Let us creat a function to easily show images.\n",
    "\n",
    "The 'torchvision.utils.make_grid' help us create a grid to show images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x122.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# data,target = dataiter.next()\n",
    "\n",
    "def imshow(batch, class_names=None, num_images=4):\n",
    "    plt.figure(figsize=(1.7 * num_images, 1.7))\n",
    "    img, classes = batch\n",
    "    \n",
    "    img_num = min(num_images, img.shape[0])\n",
    "\n",
    "    grid = torchvision.utils.make_grid(img[:img_num], \n",
    "                                       nrow=img_num, \n",
    "                                       padding = 1,\n",
    "                                      pad_value = 1)\n",
    "    grid = grid.cpu().numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    \n",
    "    plt.imshow(grid)\n",
    "    \n",
    "    if class_names:\n",
    "        titles = [class_names[x] for x in classes[:img_num]]\n",
    "        plt.axis('off')\n",
    "        plt.title(titles)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "imshow(next(iter(train_loader)), num_images=10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2abb92265a70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(117) ## random seed, use prime number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Create own Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 create own module\n",
    "\n",
    "The module consists of two parts, layers and the forward method.\n",
    "\n",
    "To build your own layers, you need to use servral functions like 'Conv2d', 'Linear', 'MaxPool2d', 'BatchNorm2d' and so on. In order to give you a quick demo, I just list some of them and you will find more information in [offical docs](https://pytorch.org/docs/stable/nn.html). **Don't forget init your layers' values.**\n",
    "\n",
    "1. 'Conv2d':\n",
    "    - definition:\n",
    "        \n",
    "        `class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)`\n",
    "\n",
    "    - There are some parameters:\n",
    "    \n",
    "        - in_channels (int): Number of channels in the input image\n",
    "        - out_channels (int): Number of channels produced by the convolution\n",
    "        - kernel_size (int or tuple): Size of the convolving kernel\n",
    "        - stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "        - padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "        - dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "        - groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "        - bias (bool, optional): If True, adds a learnable bias to the output. Default: True\n",
    "        \n",
    "2. 'Linear':\n",
    "    - definition:\n",
    "        \n",
    "        `CLASS torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "        \n",
    "    - parameters:\n",
    "    \n",
    "        - in_features: size of each input sample\n",
    "        - out_features: size of each output sample\n",
    "        - bias: If set to False, the layer will not learn an additive bias. Default: True\n",
    "        \n",
    "3. 'MaxPool2d':\n",
    "    - definition:\n",
    "        \n",
    "        `CLASS torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n",
    "        \n",
    "    - parameters:\n",
    "    \n",
    "        - kernel_size: the size of the window to take a max over\n",
    "        - stride: the stride of the window. Default value is kernel_size\n",
    "        - padding: implicit zero padding to be added on both sides\n",
    "        - dilation: a parameter that controls the stride of elements in the window\n",
    "        - return_indices: if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later\n",
    "        - ceil_mode: when True, will use ceil instead of floor to compute the output shape\n",
    "        \n",
    "4. 'BatchNorm2d':\n",
    "     - definition:\n",
    "         \n",
    "         `CLASS torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)`\n",
    "         \n",
    "     - parmaters:\n",
    "         \n",
    "        - num_features: CC from an expected input of size (N, C, H, W)(N,C,H,W)\n",
    "        - eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
    "        - momentum: the value used for the running_mean and running_var computation. Can be set to None for cumulative moving - average (i.e. simple average). Default: 0.1\n",
    "        - affine: a boolean value that when set to True, this module has learnable affine parameters. Default: True\n",
    "        - track_running_stats: a boolean value that when set to True, this module tracks the running mean and variance, and - - when set to False, this module does not track such statistics and always uses batch statistics in both training and - eval modes. Default: True\n",
    "        \n",
    "Now you have all the informations, Let's build a vgg module.\n",
    "\n",
    "**Notice, I will change some parmeters against standed vgg module.**\n",
    "\n",
    "    1. The first convolution layers's `in_channal`. As you know MNIST is just 1 * 28 * 28, It's 1 channel image, and we use MNIST to train our network, of course, will implement to digit recegnization.\n",
    "    2. We just want to classficate 10 kinds of digit, so the output layer's dimension will ended as 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, features, num_classes=10, init_weights=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "#         print('x1: ', x.size())\n",
    "#         print('x.size(0): ',x.size(0))\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print('x2: ', x.size())\n",
    "        assert( x.size() == (64,512))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "vgg16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "VGG11 = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 1\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How To Train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define our own train function, we need to seprate several part:\n",
    "    1. some informations about this epoch.\n",
    "    2. data preparing.\n",
    "    3. optimizer preparing.\n",
    "    4. forward and backward.\n",
    "    5. lost function.\n",
    "    6. show some train information.\n",
    "    7. show loss and acc Analysis image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    print('epoch: {}'.format(epoch))\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    train_list = {'loss':[],'acc':[],'idx':[]}\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm_notebook(train_loader,total=len(train_loader))):\n",
    "        \n",
    "        indx_target = targets.clone() # clone labels\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        optimizer.zero_grad() # Clears the gradients of all optimized\n",
    "#         print(inputs.size())\n",
    "        outputs = model(inputs) # output will be batch_size * class_size \n",
    "        \n",
    "        loss = F.cross_entropy(outputs, targets) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data.item()\n",
    "        \n",
    "        if batch_idx % 20 == 0 and batch_idx >= 0:\n",
    "                pred = outputs.data.max(1)[1]  # get the index of the max log-probability\n",
    "                correct = pred.cpu().eq(indx_target).sum()\n",
    "                acc = float(correct) * 1.0 / len(inputs)\n",
    "                train_list['loss'].append(loss.data.item())\n",
    "                train_list['acc'].append(acc)\n",
    "                train_list['idx'].append(batch_idx)\n",
    "                print('Train Epoch: {} [{:5}/{:5}] Loss: {:.6f} Acc: {:.4f} lr: {:.2e}'.format(\n",
    "                    epoch, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "                    loss.data.item(), acc * 100, optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "#     print(train_list['loss'],train_list['idx'])\n",
    "    # losss pic \n",
    "    plt.plot(train_list['idx'],\n",
    "             train_list['loss'],\n",
    "             \"b\",linewidth=1,\n",
    "             label = \"epoch_\" + str(epoch))   #在当前绘图对象绘图（X轴，Y轴，蓝色虚线，线宽度）\n",
    "    plt.xlabel(\"iterations\") #X轴标签\n",
    "    plt.ylabel(\"loss\")  #Y轴标签\n",
    "    plt.title(\"Lost Analysis\") #图标题\n",
    "    plt.show()\n",
    "    \n",
    "    #acc pic\n",
    "    plt.plot(train_list['idx'],\n",
    "             train_list['acc'],\n",
    "             \"r\",linewidth=1,\n",
    "             label = \"epoch_\" + str(epoch))   #在当前绘图对象绘图（X轴，Y轴，蓝色虚线，线宽度）\n",
    "    plt.xlabel(\"iterations\") #X轴标签\n",
    "    plt.ylabel(\"acc\")  #Y轴标签\n",
    "    plt.title(\"Accuracy Analysis\") #图标题\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, optimizer, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    global best_acc\n",
    "    \n",
    "    for inputs, target in tqdm_notebook(test_loader,total=len(test_loader)):\n",
    "        indx_target = target.clone()\n",
    "        \n",
    "        target = Variable(target)\n",
    "        with torch.no_grad():\n",
    "            inputs = Variable(inputs)\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "        output = model(inputs)\n",
    "        \n",
    "        test_loss += F.cross_entropy(output, target).data.item()\n",
    "        pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.cpu().eq(indx_target).sum()\n",
    "        test_loss_percent = test_loss / len(test_loader) # average over number of mini-batch\n",
    "        acc = 100. * correct / len(test_loader.dataset)\n",
    "        \n",
    "#         print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "#             test_loss_percent, correct, len(test_loader.dataset), acc))\n",
    "        \n",
    "    \n",
    "        # model save\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            save_file = 0\n",
    "            if save_file:\n",
    "                new_file = os.path.join(args.logdir, 'best-{}.pth'.format(epoch))\n",
    "                misc.model_snapshot(model, new_file, old_file=old_file, verbose=True)\n",
    "                old_file = new_file\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "    test_loss * 100 / len(test_loader), correct, len(test_loader.dataset), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(VGG11)\n",
    "model = Net(make_layers(VGG11,batch_norm = True))\n",
    "if use_cuda:\n",
    "    model.to(device)\n",
    "print(model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_acc = 0;\n",
    "try:\n",
    "    epoch_size = 1\n",
    "    for epoch in range(epoch_size):\n",
    "        train(model,device,train_loader,optimizer,epoch)\n",
    "        test(model,device,test_loader,optimizer,epoch)\n",
    "        torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\"Best Result: {:.3f}%\".format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
